{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise on the value of unsupervised constructed features for training a classifier with few labeled examples:SOLUTION\n",
    "\n",
    "To get unsupervised conStructed features of an image, we have used a pretrained CNN as feature extractor. For this purpose we pushed each image through a pretrained CNN and extracted the activations in the first fully connected layer. As pretrained CNN we use a VGG16 architecture that was trained on ImageNet data and was the second winner of the ImageNet competition in 2014.\n",
    "\n",
    "In this manner we have got unsupervised constructed features for 1000 images of the MNIST data set and 1000 images of the CIFAR10 data set. In both data sets we have 10 distinguished classes. The data sets are balanced meaning we have 100 images per class. \n",
    "\n",
    "In the last exercise we have used 2D plots to check if the feature quality is good enough to lead to visible clusters corresoonding to the 10 classes in the data. For the MNISt data it is very easy to seperate the 10 digits. Therefore we will only continue with the more challenging CIFAR10 data.\n",
    "\n",
    "As a second check on the quality of the feature representation of the CIFAR10 data, we will use once the pixel-features and once the VGG-features to train a classifier using only 100 labeled data (on average 10 per class). If the VGG-feature are indeed better, we would expect to achieve a better classifier when using the VGG-feature compared to the pixel feature.\n",
    "\n",
    "a) Which accuracy would you expect for a classifier which cannot distinguish between the 10 classes and is only guessing?\n",
    "\n",
    "**Solution: 10%**\n",
    "\n",
    "\n",
    "b) Go through the code which is used to set-up, train, and evaluate a CNN classifier using the raw pixel features. Discuss your thoughts on the achieved accuracy (e.g. with your neighbor).\n",
    "\n",
    "**Solution: The accuracy is with ~20% better then guessing but still very bad. However, this is not surprising since the resolution of the images are very low and it is alread by eye quite difficutl to distinguish between the classes. Moreover, we have  only very few training examples (only 10 per class), quite bad features (the raw pixel values) and a model with many parameters (~45k parameter).**\n",
    "\n",
    "b) Now we use the unsupervised constructed VGG features. We want to check, if these VGG features are good enough to train a classifier with only few labeled data and still get a satisfying performance. For this purpose, please complet the code to set up a fully connected NN and run the provided subsequent code to train it and determine its accuracy on the test set. Compare it to the accuracy which we achieve with a RF. Discuss the results (e.g. with your neighbor).\n",
    "\n",
    "**Solution: For code completion see below. The accuracy of the fcNN is with more than 55% much better than the accuray of the from scratch trained CNN which was 20%. This implies that the VGG-features are quite good and more informative than the raw pixel features. With the RF we do not achieve a better performance - it looks even worse than the fcNN. This is quite surprising since we would expect the fcNN to overfit the data since it has only 1000 training data and ~800 parameters. A possible reason might be that the features were constructed in the VGG model for a following fcNN classifier.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras 2.1.4 TF 1.6.0 Python sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pylab import *\n",
    "\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "import keras\n",
    "import sys\n",
    "print (\"Keras {} TF {} Python {}\".format(keras.__version__, tf.__version__, sys.version_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlad cifar data\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "del [x_test,y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over each class label and sample 100 random images over each label and save the idx to subset\n",
    "np.random.seed(seed=222)\n",
    "idx=np.empty(0,dtype=\"int8\")\n",
    "for i in range(0,len(np.unique(y_train))):\n",
    "    idx=np.append(idx,np.random.choice(np.where((y_train[0:len(y_train)])==i)[0],100,replace=False))\n",
    "\n",
    "x_train= x_train[idx]\n",
    "y_train= y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000, 1)\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([100, 100, 100, 100, 100, 100, 100, 100, 100, 100]))\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make train vaild and test\n",
    "#loop over each class label and sample 100 random images over each label and save the idx to subset\n",
    "np.random.seed(seed=123)\n",
    "idx_train=np.empty(0,dtype=\"int8\")\n",
    "for i in range(0,len(np.unique(y_train))):\n",
    "    idx_train=np.append(idx_train,np.random.choice(np.where((y_train[0:len(y_train)])==i)[0],10,replace=False))\n",
    "\n",
    "x_train_new = x_train[idx_train]\n",
    "y_train_new = y_train[idx_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_new=(np.delete(x_train,idx_train,axis=0))\n",
    "y_test_new=(np.delete(y_train,idx_train,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=127)\n",
    "idx_vaild=np.empty(0,dtype=\"int8\")\n",
    "for i in range(0,len(np.unique(y_test_new))):\n",
    "    idx_vaild=np.append(idx_vaild,np.random.choice(np.where((y_test_new[0:len(y_test_new)])==i)[0],10,replace=False))\n",
    "\n",
    "x_vaild_new = x_test_new[idx_vaild]\n",
    "y_valid_new = y_test_new[idx_vaild]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_new=(np.delete(x_test_new,idx_vaild,axis=0))\n",
    "y_test_new=(np.delete(y_test_new,idx_vaild,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = np.reshape(x_train_new, (100,32,32,3))\n",
    "x_vaild_new = np.reshape(x_vaild_new, (100,32,32,3))\n",
    "x_test_new = np.reshape(x_test_new, (800,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10]))\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10]))\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([80, 80, 80, 80, 80, 80, 80, 80, 80, 80]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_new,return_counts=True))\n",
    "print(np.unique(y_valid_new,return_counts=True))\n",
    "print(np.unique(y_test_new,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "y_train_new=to_categorical(y_train_new,10)\n",
    "y_valid_new=to_categorical(y_valid_new,10)\n",
    "y_test_new=to_categorical(y_test_new,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(x_train_new.shape)\n",
    "print(y_train_new.shape)\n",
    "\n",
    "print(x_vaild_new.shape)\n",
    "print(y_valid_new.shape)\n",
    "\n",
    "print(x_test_new.shape)\n",
    "print(y_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and standardize the data\n",
    "X_mean = np.mean( x_train_new, axis = 0)\n",
    "X_std = np.std( x_train_new, axis = 0)\n",
    "\n",
    "x_train_new = (x_train_new - X_mean ) / (X_std + 0.0001)\n",
    "x_vaild_new = (x_vaild_new - X_mean ) / (X_std + 0.0001)\n",
    "x_test_new = (x_test_new - X_mean ) / (X_std + 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the the CNN classifier based on raw image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define  hyperparameter of the NN\n",
    "batch_size = 10\n",
    "nb_classes = 10\n",
    "nb_epoch = 30\n",
    "img_rows, img_cols = 32, 32\n",
    "kernel_size = (3, 3)\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "pool_size = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 8)         224       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 8)         584       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 16)        1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 40)                41000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 46,058\n",
      "Trainable params: 45,882\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/30\n",
      " - 2s - loss: 2.7205 - acc: 0.1200 - val_loss: 2.8294 - val_acc: 0.1400\n",
      "Epoch 2/30\n",
      " - 1s - loss: 2.0518 - acc: 0.3000 - val_loss: 2.6306 - val_acc: 0.1500\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.7144 - acc: 0.4300 - val_loss: 2.4216 - val_acc: 0.1300\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.6136 - acc: 0.4300 - val_loss: 2.3575 - val_acc: 0.1200\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.3343 - acc: 0.6100 - val_loss: 2.3813 - val_acc: 0.1700\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.2349 - acc: 0.6700 - val_loss: 2.3909 - val_acc: 0.2200\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.2331 - acc: 0.6600 - val_loss: 2.4204 - val_acc: 0.1700\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.1278 - acc: 0.7500 - val_loss: 2.4020 - val_acc: 0.1600\n",
      "Epoch 9/30\n",
      " - 1s - loss: 0.8816 - acc: 0.8100 - val_loss: 2.3726 - val_acc: 0.1900\n",
      "Epoch 10/30\n",
      " - 1s - loss: 0.8182 - acc: 0.8400 - val_loss: 2.3940 - val_acc: 0.2300\n",
      "Epoch 11/30\n",
      " - 1s - loss: 0.7492 - acc: 0.8500 - val_loss: 2.3765 - val_acc: 0.2200\n",
      "Epoch 12/30\n",
      " - 1s - loss: 0.7999 - acc: 0.8700 - val_loss: 2.3246 - val_acc: 0.2100\n",
      "Epoch 13/30\n",
      " - 1s - loss: 0.6810 - acc: 0.9000 - val_loss: 2.3531 - val_acc: 0.2300\n",
      "Epoch 14/30\n",
      " - 1s - loss: 0.6203 - acc: 0.9600 - val_loss: 2.3716 - val_acc: 0.1900\n",
      "Epoch 15/30\n",
      " - 1s - loss: 0.4948 - acc: 0.9700 - val_loss: 2.3512 - val_acc: 0.1600\n",
      "Epoch 16/30\n",
      " - 1s - loss: 0.5716 - acc: 0.9200 - val_loss: 2.3341 - val_acc: 0.1900\n",
      "Epoch 17/30\n",
      " - 1s - loss: 0.5000 - acc: 0.9800 - val_loss: 2.3725 - val_acc: 0.2100\n",
      "Epoch 18/30\n",
      " - 1s - loss: 0.4506 - acc: 0.9700 - val_loss: 2.3948 - val_acc: 0.2100\n",
      "Epoch 19/30\n",
      " - 1s - loss: 0.4416 - acc: 0.9500 - val_loss: 2.4000 - val_acc: 0.1800\n",
      "Epoch 20/30\n",
      " - 1s - loss: 0.3382 - acc: 0.9800 - val_loss: 2.4519 - val_acc: 0.1700\n",
      "Epoch 21/30\n",
      " - 1s - loss: 0.4426 - acc: 0.9500 - val_loss: 2.4555 - val_acc: 0.1400\n",
      "Epoch 22/30\n",
      " - 1s - loss: 0.3712 - acc: 0.9800 - val_loss: 2.3453 - val_acc: 0.1500\n",
      "Epoch 23/30\n",
      " - 1s - loss: 0.3722 - acc: 0.9900 - val_loss: 2.3546 - val_acc: 0.1700\n",
      "Epoch 24/30\n",
      " - 1s - loss: 0.3226 - acc: 0.9900 - val_loss: 2.3764 - val_acc: 0.1900\n",
      "Epoch 25/30\n",
      " - 1s - loss: 0.3086 - acc: 0.9900 - val_loss: 2.4076 - val_acc: 0.1900\n",
      "Epoch 26/30\n",
      " - 1s - loss: 0.3314 - acc: 0.9900 - val_loss: 2.4050 - val_acc: 0.1900\n",
      "Epoch 27/30\n",
      " - 1s - loss: 0.3004 - acc: 0.9900 - val_loss: 2.4735 - val_acc: 0.1900\n",
      "Epoch 28/30\n",
      " - 1s - loss: 0.2936 - acc: 0.9800 - val_loss: 2.4725 - val_acc: 0.1500\n",
      "Epoch 29/30\n",
      " - 1s - loss: 0.2869 - acc: 0.9800 - val_loss: 2.4478 - val_acc: 0.1800\n",
      "Epoch 30/30\n",
      " - 1s - loss: 0.2342 - acc: 0.9600 - val_loss: 2.4781 - val_acc: 0.1900\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train_new, y_train_new, \n",
    "                  batch_size=10, \n",
    "                  epochs=30,\n",
    "                  verbose=2, \n",
    "                  validation_data=(x_vaild_new, y_valid_new),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the CNN classifier that was trained on raw image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28  2  9  2  8  3  3  9 16  0]\n",
      " [11  5 11  2  9  9  1  2 16 14]\n",
      " [ 8  2 17  2 17  7 13  4  8  2]\n",
      " [ 4  1 20  7 10 14  9  5  6  4]\n",
      " [ 5  1 16  6 14  3 20 10  4  1]\n",
      " [ 3  2 13  5 11 13 10 15  4  4]\n",
      " [ 2  1 14  5 10 17 23  2  3  3]\n",
      " [ 2  0  9  2 22  9  5 25  3  3]\n",
      " [21  3  5  1 10  6  1  5 23  5]\n",
      " [ 5  7  9  1 10 13  2  3 15 15]]\n",
      "Acc =  0.2125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred=model.predict(x_test_new)\n",
    "print(confusion_matrix(np.argmax(y_test_new,axis=1),np.argmax(pred,axis=1)))\n",
    "print(\"Acc = \" ,np.sum(np.argmax(y_test_new,axis=1)==np.argmax(pred,axis=1))/len(y_test_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the VGG features for CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 18M Mar 29 12:35 cifar_EMB_1000.npz\r\n"
     ]
    }
   ],
   "source": [
    "# Downloading embeddings\n",
    "import urllib\n",
    "import os\n",
    "if not os.path.isfile('cifar_EMB_1000.npz'):\n",
    "    urllib.request.urlretrieve(\n",
    "    \"https://www.dropbox.com/s/si287al91c1ls0d/cifar_EMB_1000.npz?dl=1\",\n",
    "    \"cifar_EMB_1000.npz\")\n",
    "%ls -hl cifar_EMB_1000.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=np.load(\"cifar_EMB_1000.npz\")\n",
    "vgg_features_cifar = Data[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_features_cifar_train = vgg_features_cifar[idx_train]\n",
    "vgg_features_cifar_test=(np.delete(vgg_features_cifar,idx_train,axis=0))\n",
    "vgg_features_cifar_valid = vgg_features_cifar_test[idx_vaild]\n",
    "vgg_features_cifar_test=(np.delete(vgg_features_cifar_test,idx_vaild,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4096)\n",
      "(100, 4096)\n",
      "(800, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(vgg_features_cifar_train.shape)\n",
    "print(vgg_features_cifar_valid.shape)\n",
    "print(vgg_features_cifar_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the the CNN classifier based on VGG feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200,batch_input_shape=(None, 4096)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200,input_shape=(None,4096)))\n",
    "\n",
    "#### we still need to add the last layers to get the predictions on the 10 classes\n",
    "### your code here\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "####### end of your code ######\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 200)               819400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 862,410\n",
      "Trainable params: 862,010\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 2.3676 - acc: 0.2700 - val_loss: 2.5823 - val_acc: 0.3700\n",
      "Epoch 2/20\n",
      " - 0s - loss: 0.8896 - acc: 0.7200 - val_loss: 2.1334 - val_acc: 0.4300\n",
      "Epoch 3/20\n",
      " - 0s - loss: 0.5962 - acc: 0.7800 - val_loss: 1.6625 - val_acc: 0.5100\n",
      "Epoch 4/20\n",
      " - 0s - loss: 0.4425 - acc: 0.8900 - val_loss: 1.7256 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      " - 0s - loss: 0.4135 - acc: 0.8800 - val_loss: 1.7016 - val_acc: 0.5200\n",
      "Epoch 6/20\n",
      " - 0s - loss: 0.2874 - acc: 0.9100 - val_loss: 1.6401 - val_acc: 0.5400\n",
      "Epoch 7/20\n",
      " - 0s - loss: 0.2953 - acc: 0.9000 - val_loss: 1.5616 - val_acc: 0.5400\n",
      "Epoch 8/20\n",
      " - 0s - loss: 0.2427 - acc: 0.9200 - val_loss: 1.5743 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      " - 0s - loss: 0.2008 - acc: 0.9500 - val_loss: 1.6489 - val_acc: 0.5200\n",
      "Epoch 10/20\n",
      " - 0s - loss: 0.1446 - acc: 0.9500 - val_loss: 1.6543 - val_acc: 0.4900\n",
      "Epoch 11/20\n",
      " - 0s - loss: 0.1986 - acc: 0.9300 - val_loss: 1.7274 - val_acc: 0.4900\n",
      "Epoch 12/20\n",
      " - 0s - loss: 0.0897 - acc: 0.9800 - val_loss: 1.8760 - val_acc: 0.4900\n",
      "Epoch 13/20\n",
      " - 0s - loss: 0.1229 - acc: 0.9800 - val_loss: 1.8831 - val_acc: 0.4900\n",
      "Epoch 14/20\n",
      " - 0s - loss: 0.1261 - acc: 0.9600 - val_loss: 1.8859 - val_acc: 0.4700\n",
      "Epoch 15/20\n",
      " - 0s - loss: 0.0721 - acc: 0.9900 - val_loss: 1.7835 - val_acc: 0.5200\n",
      "Epoch 16/20\n",
      " - 0s - loss: 0.0702 - acc: 0.9900 - val_loss: 1.7420 - val_acc: 0.5300\n",
      "Epoch 17/20\n",
      " - 0s - loss: 0.1151 - acc: 0.9800 - val_loss: 1.7813 - val_acc: 0.5300\n",
      "Epoch 18/20\n",
      " - 0s - loss: 0.0507 - acc: 0.9800 - val_loss: 1.8089 - val_acc: 0.5700\n",
      "Epoch 19/20\n",
      " - 0s - loss: 0.0739 - acc: 0.9700 - val_loss: 1.8342 - val_acc: 0.5500\n",
      "Epoch 20/20\n",
      " - 0s - loss: 0.0494 - acc: 0.9900 - val_loss: 1.9215 - val_acc: 0.5200\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(vgg_features_cifar_train, y_train_new, \n",
    "                  batch_size=10, \n",
    "                  epochs=20,\n",
    "                  verbose=2, \n",
    "                  validation_data=(vgg_features_cifar_valid, y_valid_new),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the CNN classifier that was trained on VGG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  1  2  1  0  0  1  2 20  4]\n",
      " [ 6 55  0  0  0  1  1  3  4 10]\n",
      " [14  0 31  1 13  8  5  1  6  1]\n",
      " [ 4  0  1 16  1 23 18  2  9  6]\n",
      " [ 4  0  8  2 37  3  4 17  5  0]\n",
      " [ 2  0  0 11  6 46  6  8  0  1]\n",
      " [ 5  0 26  2  6  2 37  0  2  0]\n",
      " [ 3  0  1  3 13  4  2 52  1  1]\n",
      " [11  1  0  0  0  0  0  0 67  1]\n",
      " [ 4 14  0  0  1  1  0  0  5 55]]\n",
      "Acc =  0.55625\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict(vgg_features_cifar_test)\n",
    "\n",
    "#### we now want to get the confusion matrix for the predictions on the test data\n",
    "### your code here\n",
    "\n",
    "print(confusion_matrix(np.argmax(y_test_new,axis=1),np.argmax(pred,axis=1)))\n",
    "print(\"Acc = \" ,np.sum(np.argmax(y_test_new,axis=1)==np.argmax(pred,axis=1))/len(y_test_new))\n",
    "\n",
    "########## end of your code ###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: use VGG feature to train a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(vgg_features_cifar_train,np.argmax(y_train_new, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  5  8  2  2  1  2  6 21  3]\n",
      " [ 7 40  0  0  3  8  1  1  8 12]\n",
      " [ 6  0 32 12  7  5  2 11  3  2]\n",
      " [ 3  8 18 20  3  8  7  6  5  2]\n",
      " [ 4  2 25 10 15  2  6 11  4  1]\n",
      " [ 3  2 10 16  3 39  2  4  1  0]\n",
      " [ 7  2 31  7  6  9  7  6  4  1]\n",
      " [ 5  2 11  5 13  7  1 31  4  1]\n",
      " [10  7  8  1  2  5  1  4 38  4]\n",
      " [ 4 16  1  1  1  4  1  2  8 42]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3675"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred=clf.predict(vgg_features_cifar_test)\n",
    "print(confusion_matrix(np.argmax(y_test_new, axis=1), pred))\n",
    "np.sum(pred==np.argmax(y_test_new, axis=1))/len(np.argmax(y_test_new, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
